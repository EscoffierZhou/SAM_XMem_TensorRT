import os
# === 0. ç¯å¢ƒå˜é‡ä¼˜åŒ– ===
# å¼€å¯æ˜¾å­˜ç¢ç‰‡æ•´ç†ï¼Œè¿™å¯¹ 8GB æ˜¾å­˜è‡³å…³é‡è¦
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import sys
import torch
import numpy as np
import cv2
import gc
import time
import gradio as gr
from PIL import Image

# === 1. è·¯å¾„ä¸ç¯å¢ƒé…ç½® ===
sys.path.append(os.path.join(os.getcwd(), 'XMem'))

from model.network import XMem
from inference.inference_core import InferenceCore
from segment_anything import sam_model_registry, SamPredictor

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ğŸš€ Running on: {device}")

# === 2. æ¨¡å‹åŠ è½½ ===

# --- åŠ è½½ XMem (FP32 + Autocast) ---
print("Loading XMem model...")
# ä¿æŒ FP32 æƒé‡ï¼Œä¾é  Autocast å’Œ no_grad ä¼˜åŒ–æ˜¾å­˜
network = XMem(config={'enable_long_term': True, 'enable_short_term': True}).to(device).eval()
xmem_checkpoint = 'XMem.pth'

def load_xmem_weights(model, path):
    if not os.path.exists(path):
        print(f"âŒ Error: Weights not found at {path}")
        return
    try:
        checkpoint = torch.load(path, map_location='cpu')
        if 'state_dict' in checkpoint: state_dict = checkpoint['state_dict']
        elif 'model' in checkpoint: state_dict = checkpoint['model']
        else: state_dict = checkpoint
        model.load_state_dict(state_dict, strict=False)
        print(f"âœ… XMem weights loaded successfully from {path}")
    except Exception as e:
        print(f"âŒ Failed to load XMem weights: {e}")

load_xmem_weights(network, xmem_checkpoint)

# --- åŠ è½½ SAM ---
print("Loading SAM model...")
sam_checkpoint = "sam_vit_b_01ec64.pth"
sam = sam_model_registry["vit_b"](checkpoint=sam_checkpoint)
sam.to(device=device)
predictor = SamPredictor(sam)
print("âœ… SAM Loaded!")

# === 3. å…¨å±€çŠ¶æ€ ===
class TrackerState:
    def __init__(self):
        self.cap = None
        self.first_frame = None
        self.mask = None
        self.video_path = None

state = TrackerState()

# === 4. æ ¸å¿ƒé€»è¾‘ ===

def on_video_upload(video_path):
    if video_path is None: return None

    state.video_path = video_path
    state.cap = cv2.VideoCapture(video_path)
    ret, frame = state.cap.read()
    if not ret: return None

    # BGR -> RGB
    state.first_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    print("âš¡ Extracting SAM features for the first frame...")
    predictor.set_image(state.first_frame)
    state.mask = None
    return state.first_frame

def on_click(evt: gr.SelectData):
    if state.first_frame is None: return None
    print(f"ğŸ–±ï¸ Clicked at: {evt.index}")

    try:
        input_point = np.array([[evt.index[0], evt.index[1]]])
        input_label = np.array([1])

        masks, scores, _ = predictor.predict(
            point_coords=input_point,
            point_labels=input_label,
            multimask_output=False,
        )

        best_mask = masks[0]
        state.mask = best_mask.astype(np.uint8) * 255

        # å¯è§†åŒ–
        overlay = state.first_frame.copy()
        red_map = np.zeros_like(overlay)
        red_map[:, :, 0] = 255

        alpha = 0.5
        mask_bool = state.mask > 0
        if mask_bool.any():
            overlay[mask_bool] = cv2.addWeighted(
                overlay[mask_bool], 1 - alpha,
                red_map[mask_bool], alpha, 0
            ).squeeze()

        cv2.circle(overlay, (evt.index[0], evt.index[1]), 6, (0, 255, 0), -1)
        return overlay

    except Exception as e:
        print(f"âŒ SAM Prediction Error: {e}")
        return state.first_frame

@torch.no_grad()
def run_tracking(progress=gr.Progress()):
    if state.mask is None or state.cap is None:
        return None, "âŒ Error: No mask or video loaded."

    print("ğŸš€ Starting XMem Tracking Pipeline...")

    # æ€§èƒ½ç›‘æ§åˆå§‹åŒ–
    start_time = time.time()
    torch.cuda.reset_peak_memory_stats()

    # æ˜¾å­˜æ¸…ç†
    torch.cuda.empty_cache()
    gc.collect()

    # åˆå§‹åŒ–æ¨ç†æ ¸å¿ƒ
    processor = InferenceCore(network, config={
        'enable_long_term': True,
        'enable_short_term': True,
        'enable_long_term_count_usage': True,
        'hidden_dim': 64,
        'key_dim': 64,
        'value_dim': 512,
        'num_prototypes': 128,
        'min_mid_term_frames': 5,      # æ”¹å› 5
        'max_mid_term_frames': 10,     # æ”¹å› 10
        'max_long_term_elements': 10000, # æ”¹å› 10000 (è®©å®ƒè®°å¾—æ›´ä¹…)
        'mem_every': 5,
        'deep_update_every': -1,
        'save_every': -1,
        'show_every': -1,
        'size': -1,
        'top_k': 30,
    })

    # ä¼ å…¥ç¬¬ä¸€å¸§å’Œ Mask
    mask_torch = torch.from_numpy(state.mask > 128).long().to(device)
    processor.set_all_labels([1])

    frame_torch = (torch.from_numpy(state.first_frame).permute(2, 0, 1).float().to(device) / 255.0)

    with torch.autocast("cuda"):
        processor.step(frame_torch, mask_torch[None, ...])

    # è§†é¢‘å†™å…¥è®¾ç½®
    width = int(state.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(state.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = state.cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(state.cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # å°è¯•ä½¿ç”¨æµè§ˆå™¨å…¼å®¹æ€§æ›´å¥½çš„ç¼–ç 
    # H.264 (avc1) > VP9 (vp09) > MP4V (å…¼å®¹æ€§æœ€å·®ä½†æ— éœ€é¢å¤–åº“)
    output_path = "tracking_result.mp4"
    codecs_to_try = ['avc1', 'vp09', 'mp4v']
    writer = None

    for codec in codecs_to_try:
        try:
            fourcc = cv2.VideoWriter_fourcc(*codec)
            writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            if writer.isOpened():
                print(f"âœ… Using video codec: {codec}")
                break
        except:
            continue

    if writer is None or not writer.isOpened():
        print("âš ï¸ Failed to initialize preferred codecs, falling back to default mp4v")
        writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))

    state.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

    # å¤„ç†å¾ªç¯
    for idx in progress.tqdm(range(total_frames), desc="Processing"):
        ret, frame = state.cap.read()
        if not ret: break

        vis = frame.copy()

        if idx == 0:
            pred_mask = (state.mask > 0)
        else:
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_tensor = (torch.from_numpy(frame_rgb).permute(2, 0, 1).float().to(device) / 255.0)

            with torch.autocast("cuda"):
                prob = processor.step(frame_tensor)

            pred_mask = torch.argmax(prob, dim=0)
            pred_mask = (pred_mask.cpu().numpy() == 1)

        if pred_mask.any():
            green_map = np.zeros_like(vis)
            green_map[:, :, 1] = 255
            alpha = 0.5
            vis[pred_mask] = cv2.addWeighted(
                vis[pred_mask], 1-alpha,
                green_map[pred_mask], alpha, 0
            ).squeeze()

        writer.write(vis)

        if idx % 20 == 0:
            torch.cuda.empty_cache()

    writer.release()
    del processor
    torch.cuda.empty_cache()
    gc.collect()

    # === æ€§èƒ½ç»Ÿè®¡ ===
    end_time = time.time()
    total_time = end_time - start_time
    avg_fps = total_frames / total_time if total_time > 0 else 0
    max_mem = torch.cuda.max_memory_allocated() / (1024 ** 2) # MB

    metrics_str = (
        f"âœ… Tracking Completed!\n"
        f"â±ï¸ Total Time: {total_time:.2f} s\n"
        f"ğŸï¸ Total Frames: {total_frames}\n"
        f"ğŸš€ Average FPS: {avg_fps:.2f} fps\n"
        f"ğŸ’¾ Peak GPU Memory: {max_mem:.2f} MB"
    )
    print(metrics_str)

    return output_path, metrics_str

# === 5. UI ===
with gr.Blocks(title="SAM_Xmem Tracker (TensoRTä¼˜åŒ–)") as demo:
    gr.Markdown("# SAM_Xmem Tracker (TensoRTä¼˜åŒ–)")

    with gr.Row():
        # å·¦ä¾§ï¼šäº¤äº’åŒº
        with gr.Column():
            gr.Markdown("### 1. ä¸Šä¼ ä¸é€‰æ‹©")
            video_in = gr.Video(label="ä¸Šä¼ è§†é¢‘")
            click_img = gr.Image(label="ç‚¹å‡»é€‰æ‹©å¯¹è±¡", interactive=True, type="numpy")

        # å³ä¾§ï¼šç»“æœåŒº
        with gr.Column():
            gr.Markdown("### 2. ç»“æœé¢„è§ˆ")
            mask_preview = gr.Image(label="Mask é¢„è§ˆ", interactive=False)
            track_btn = gr.Button("ğŸš€ å¼€å§‹è¿½è¸ª (ä¿ç•™åŸç”»è´¨)", variant="primary")

            # è¾“å‡ºåŒºåŸŸï¼šè§†é¢‘ + æŒ‡æ ‡
            video_out = gr.Video(label="è¿½è¸ªç»“æœ")
            metrics_out = gr.Textbox(label="æ€§èƒ½æŒ‡æ ‡ (Performance Metrics)", lines=5)

    # äº‹ä»¶ç»‘å®š
    video_in.upload(on_video_upload, inputs=video_in, outputs=click_img)
    click_img.select(on_click, inputs=None, outputs=mask_preview)

    # ä¿®æ”¹ï¼šåŒæ—¶è¾“å‡º è§†é¢‘ å’Œ æŒ‡æ ‡æ–‡æœ¬
    track_btn.click(run_tracking, inputs=None, outputs=[video_out, metrics_out])

if __name__ == "__main__":
    # å…è®¸å±€åŸŸç½‘è®¿é—®
    demo.launch(server_name="0.0.0.0", server_port=7860)